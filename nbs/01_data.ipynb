{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1f11d13-2015-48fb-bdf4-ccc0ed9f38a8",
   "metadata": {},
   "source": [
    "# data\n",
    "\n",
    "The `data` module handles preprocesing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ccb303-2357-48d9-9926-32cf98b2d1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e9f112-0955-42aa-b1e5-a8c321b472dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a27ee0f-5372-4b3c-a458-1e848ec73ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "class Data:\n",
    "    def __init__(self, inputs, outputs, model_id=\"google/flan-t5-base\", verbose=1):\n",
    "        \"\"\"\n",
    "        Preprocess a sequence-to-sequence dataset\n",
    "        \"\"\"\n",
    "        self.ins = inputs\n",
    "        self.outs = outputs\n",
    "        self.verbose = 1\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        self.tokenized_inputs, self.tokenized_outputs = self.truncate()\n",
    "        \n",
    "    def truncate(self):\n",
    "        \"\"\"\n",
    "        Truncate input and target texts.\n",
    "        \"\"\"\n",
    "        ins, outs = Dataset.from_dict({'text': self.ins}), Dataset.from_dict({'text':self.outs})\n",
    "        \n",
    "        \n",
    "        tokenized_inputs = ins.map(lambda x: self.tokenizer(x[\"text\"], truncation=True), batched=True, remove_columns=[\"text\"])\n",
    "        max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n",
    "        if self.verbose: print(f\"Max source length: {max_source_length}\")\n",
    "\n",
    "        # The maximum total sequence length for target text after tokenization. \n",
    "        # Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
    "        tokenized_targets = outs.map(lambda x: self.tokenizer(x[\"text\"], truncation=True), batched=True, remove_columns=[\"text\"])\n",
    "        max_target_length = max([len(x) for x in tokenized_targets[\"input_ids\"]])\n",
    "        if self.verbose: print(f\"Max target length: {max_target_length}\")\n",
    "        return tokenized_inputs, tokenized_targets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064ff774-dccb-489a-999e-9d2aea90d271",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset samsum (/root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8091566f38a5447dbd66cb20b27b8fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_id = \"samsum\"\n",
    "dataset = load_dataset(dataset_id)\n",
    "inputs = [row['dialogue'] for row in concatenate_datasets([dataset[\"train\"], dataset[\"test\"]])]\n",
    "outputs = [row['summary'] for row in concatenate_datasets([dataset[\"train\"], dataset[\"test\"]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ecab04-d03f-48bd-a394-4083d2e14645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15551 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max source length: 512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15551 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max target length: 95\n"
     ]
    }
   ],
   "source": [
    "ds = Data(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8f0ef1-4b72-4723-8197-58f0d9be860c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(max([len(x) for x in ds.tokenized_outputs[\"input_ids\"]]) == 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17edc364-4c6a-4536-aa31-9b65563f1f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a473b45f-010b-4a35-86b7-899c1592b202",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
